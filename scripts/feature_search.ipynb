{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "132ed2a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, balanced_accuracy_score, f1_score\n",
    "from itertools import combinations\n",
    "from tqdm.notebook import tqdm\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "98376e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(df: pd.DataFrame, target_col: str='FL_UDSD', diagnosis_order: list=None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Preprocess the data by splitting into train and test sets.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): The input dataframe.\n",
    "        random_state (int): Random state for reproducibility.\n",
    "        target_col (str): The target column for stratification.\n",
    "    \n",
    "    Returns:\n",
    "        Tuple[pd.DataFrame, pd.DataFrame]: The train and test dataframes.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Clean data\n",
    "    filter_df = df[df[target_col] != 'Unknown'] # Remove rows with unknown target values\n",
    "    filter_df = filter_df[filter_df[\"MMSE\"] != -1] # Remove rows with invalid MMSE values\n",
    "    \n",
    "    # Convert columns to categorical if needed\n",
    "    filter_df['APOE'] = filter_df['APOE'].astype('category')\n",
    "    filter_df['AMYLPET'] = filter_df['AMYLPET'].astype('category')\n",
    "    \n",
    "    # Encode the target variable as an ordered categorical variable.    \n",
    "    filter_df['FL_UDSD'] = pd.Categorical(filter_df['FL_UDSD'], categories=diagnosis_order, ordered=True)\n",
    "    filter_df['FL_UDSD_cat'] = filter_df['FL_UDSD'].cat.codes \n",
    "    # filter_df.drop(columns=['FL_UDSD'], inplace=True)\n",
    "    \n",
    "    return filter_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd838924",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_categories(df: pd.DataFrame, combination_map: dict, target_col: str = 'FL_UDSD') -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Combine multiple categories in the target column into single categories.\n",
    "    \n",
    "    Args:\n",
    "        df: Input dataframe\n",
    "        target_col: Column containing categories to combine\n",
    "        combination_map: Dictionary where keys are new category names and values are lists of \n",
    "                        categories to combine into that new category.\n",
    "                        Example: {'SCD/Impaired': ['Subjective Cognitive Decline', 'Impaired Not SCD/MCI'],\n",
    "                                 'Normal/SCD': ['Normal cognition', 'Subjective Cognitive Decline']}\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with combined categories\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Apply each combination\n",
    "    for new_category, old_categories in combination_map.items():\n",
    "        df[target_col] = df[target_col].replace(old_categories, new_category)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9ed976cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10000 entries, 0 to 9999\n",
      "Data columns (total 11 columns):\n",
      " #   Column              Non-Null Count  Dtype  \n",
      "---  ------              --------------  -----  \n",
      " 0   MMSE                10000 non-null  int64  \n",
      " 1   CDRSUM              10000 non-null  float64\n",
      " 2   CDRGLOB             10000 non-null  float64\n",
      " 3   HVLT_DR             8696 non-null   float64\n",
      " 4   LASSI_A_CR2         8668 non-null   float64\n",
      " 5   LASSI_B_CR1         8607 non-null   float64\n",
      " 6   LASSI_B_CR2         8499 non-null   float64\n",
      " 7   APOE                8674 non-null   float64\n",
      " 8   AMYLPET             6791 non-null   float64\n",
      " 9   PTAU_217_CONCNTRTN  3017 non-null   float64\n",
      " 10  FL_UDSD             10000 non-null  object \n",
      "dtypes: float64(9), int64(1), object(1)\n",
      "memory usage: 859.5+ KB\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('../data/synthetic_data.csv')\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d827048b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# diagnosis_order = ['Normal cognition', 'Subjective Cognitive Decline', 'Impaired Not SCD/MCI',\n",
    "#                        'Early MCI', 'Late MCI', 'Dementia']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "68d81364",
   "metadata": {},
   "outputs": [],
   "source": [
    "combination_map = {\n",
    "    'SCD/Impaired': ['Subjective Cognitive Decline', 'Impaired Not SCD/MCI'],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "74314f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "diagnosis_order = [\n",
    "    'Normal cognition', \n",
    "    'SCD/Impaired',\n",
    "    'Early MCI', \n",
    "    'Late MCI', \n",
    "    'Dementia'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "acf06ded",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FL_UDSD\n",
       "Early MCI                       2852\n",
       "Subjective Cognitive Decline    1821\n",
       "Normal cognition                1693\n",
       "Dementia                        1668\n",
       "Late MCI                        1224\n",
       "Impaired Not SCD/MCI             742\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.value_counts('FL_UDSD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "707b8018",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = combine_categories(df, combination_map, target_col='FL_UDSD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "93759295",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FL_UDSD\n",
       "Early MCI           2852\n",
       "SCD/Impaired        2563\n",
       "Normal cognition    1693\n",
       "Dementia            1668\n",
       "Late MCI            1224\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.value_counts('FL_UDSD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "46d54697",
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_df = preprocess_data(df= df, diagnosis_order=diagnosis_order)\n",
    "filter_df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ed23c554",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FL_UDSD\n",
       "Early MCI           345\n",
       "Dementia            202\n",
       "Late MCI            140\n",
       "SCD/Impaired        136\n",
       "Normal cognition     72\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filter_df['FL_UDSD'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c58eb7d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_df.drop(columns=['FL_UDSD'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "710d4f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = train_test_split(filter_df, test_size=0.2, random_state=42, stratify=filter_df['FL_UDSD_cat'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8a063bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(y_true, y_pred) -> dict:\n",
    "    \"\"\"\n",
    "    Calculate evaluation metrics for a given set of true and predicted labels.\n",
    "    \n",
    "    Args:\n",
    "        y_true: True labels\n",
    "        y_pred: Predicted labels\n",
    "    \n",
    "    Returns:\n",
    "        mertrics: A dictionary containing accuracy, balanced accuracy, and f1 score.\n",
    "    \"\"\"\n",
    "    \n",
    "    metrics = {\n",
    "        'accuracy': accuracy_score(y_true, y_pred),\n",
    "        'balanced_accuracy': balanced_accuracy_score(y_true, y_pred),\n",
    "        'f1_score_macro': f1_score(y_true, y_pred, average='macro')\n",
    "    }\n",
    "    \n",
    "    metrics = {k: float(f'{v:.5f}') for k, v in metrics.items()}\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "72670ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate_model(\n",
    "    train_df: pd.DataFrame,\n",
    "    test_df: pd.DataFrame,\n",
    "    model,\n",
    "    target_col: str = 'FL_UDSD_cat'\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Train and evaluate a single model.\n",
    "    \n",
    "    Args:\n",
    "        train_df: Training dataframe\n",
    "        test_df: Test dataframe\n",
    "        model: An instance of a sklearn model to train and evaluate\n",
    "        target_col: Name of target column to exclude from features (default: 'FL_UDSD_cat')\n",
    "    \n",
    "    returns:\n",
    "        dict: A dictionary containing training and testing metrics (accuracy, balanced accuracy, f1 score) for both train and test sets.\n",
    "    \n",
    "    \"\"\"\n",
    "    X_train = train_df.drop(columns=[target_col])\n",
    "    y_train = train_df[target_col]\n",
    "    X_test = test_df.drop(columns=[target_col])\n",
    "    y_test = test_df[target_col]\n",
    "    \n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    train_preds = model.predict(X_train)\n",
    "    test_preds = model.predict(X_test)\n",
    "    \n",
    "    train_metrics = calculate_metrics(y_train, train_preds)\n",
    "    test_metrics = calculate_metrics(y_test, test_preds)\n",
    "    \n",
    "    metrics = {\n",
    "        'train_accuracy': train_metrics['accuracy'],\n",
    "        'test_accuracy': test_metrics['accuracy'],\n",
    "        'train_balanced_accuracy': train_metrics['balanced_accuracy'],\n",
    "        'test_balanced_accuracy': test_metrics['balanced_accuracy'],\n",
    "        'train_f1_macro_score': train_metrics['f1_score_macro'],\n",
    "        'test_f1_macro_score': test_metrics['f1_score_macro']\n",
    "    }\n",
    "    \n",
    "    return metrics\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "533155b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_results = train_and_evaluate_model(\n",
    "    train_df=train_df,\n",
    "    test_df=test_df,\n",
    "    model=RandomForestClassifier(random_state=42)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9f7eabe2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train_accuracy': 1.0,\n",
       " 'test_accuracy': 0.6257,\n",
       " 'train_balanced_accuracy': 1.0,\n",
       " 'test_balanced_accuracy': 0.54141,\n",
       " 'train_f1_macro_score': 1.0,\n",
       " 'test_f1_macro_score': 0.5449}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "39c84d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_results = train_and_evaluate_model(\n",
    "    train_df=train_df,\n",
    "    test_df=test_df,\n",
    "    model= LogisticRegression(max_iter=10000, random_state=42)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9dc85b19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train_accuracy': 0.5824,\n",
       " 'test_accuracy': 0.62011,\n",
       " 'train_balanced_accuracy': 0.46621,\n",
       " 'test_balanced_accuracy': 0.52617,\n",
       " 'train_f1_macro_score': 0.47563,\n",
       " 'test_f1_macro_score': 0.53151}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ae3e3707",
   "metadata": {},
   "outputs": [],
   "source": [
    "def comprehensive_feature_search(\n",
    "    train_df: pd.DataFrame,\n",
    "    test_df: pd.DataFrame,\n",
    "    target_col: str = 'FL_UDSD_cat',\n",
    "    min_features: int = 2,\n",
    "    max_features: int = None,\n",
    "    models: dict = None\n",
    "):\n",
    "    \"\"\"\n",
    "    Test all possible feature subsets with multiple models.\n",
    "    \n",
    "    Args:\n",
    "        train_df: Training dataframe\n",
    "        test_df: Test dataframe\n",
    "        target_col: Name of target column to exclude from features\n",
    "        min_features: Minimum number of features in a subset (default: 2)\n",
    "        max_features: Maximum number of features in a subset (default: all features)\n",
    "        models: Dictionary of model names and their corresponding sklearn model instances\n",
    "            ie. {'RandomForest': RandomForestClassifier(random_state=42), 'LogisticRegression': LogisticRegression(max_iter=10000, random_state=42)}\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: Results sorted by test balanced accuracy\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get all available features (excluding target column)\n",
    "    all_features = [col for col in train_df.columns if col != target_col]\n",
    "    \n",
    "    if max_features is None:\n",
    "        max_features = len(all_features)\n",
    "    \n",
    "    print(f\"Total features available: {len(all_features)}\")\n",
    "    print(f\"Features: {all_features}\\n\")\n",
    "    \n",
    "    # Define models to test\n",
    "    if models is None:\n",
    "        models = {\n",
    "            'RandomForest': RandomForestClassifier(random_state=42, n_estimators=100),\n",
    "            'LogisticRegression': LogisticRegression(max_iter=10000, random_state=42)\n",
    "        }\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    # Try all subset sizes from min_features to max_features\n",
    "    for n_features in range(min_features, max_features + 1):\n",
    "        # Generate all combinations of n_features\n",
    "        feature_combinations = list(combinations(all_features, n_features))\n",
    "        \n",
    "        print(f\"Testing {len(feature_combinations)} combinations with {n_features} features...\")\n",
    "        \n",
    "        # Test each combination with each model\n",
    "        for features in tqdm(feature_combinations, desc=f\"{n_features} features\"):\n",
    "            features = list(features)\n",
    "            \n",
    "            # Prepare data with selected features\n",
    "            X_train = train_df[features]\n",
    "            y_train = train_df[target_col]\n",
    "            X_test = test_df[features]\n",
    "            y_test = test_df[target_col]\n",
    "            \n",
    "            # Test each model\n",
    "            for model_name, model in models.items():\n",
    "                try:\n",
    "                    # Train model\n",
    "                    model.fit(X_train, y_train)\n",
    "                    \n",
    "                    # Make predictions\n",
    "                    train_preds = model.predict(X_train)\n",
    "                    test_preds = model.predict(X_test)\n",
    "                    \n",
    "                    train_metrics = calculate_metrics(y_train, train_preds)\n",
    "                    test_metrics = calculate_metrics(y_test, test_preds)\n",
    "                    \n",
    "                    # Calculate metrics\n",
    "                    result = {\n",
    "                        'model': model_name,\n",
    "                        'n_features': n_features,\n",
    "                        'features': ', '.join(features),\n",
    "                        'train_accuracy': train_metrics['accuracy'],\n",
    "                        'test_accuracy': test_metrics['accuracy'],\n",
    "                        'train_balanced_acc': train_metrics['balanced_accuracy'],\n",
    "                        'test_balanced_acc': test_metrics['balanced_accuracy'],\n",
    "                        'train_f1_macro_score': train_metrics['f1_score_macro'],\n",
    "                        'test_f1_macro_score': test_metrics['f1_score_macro']\n",
    "                    }\n",
    "                    \n",
    "                    results.append(result)\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    # Skip combinations that cause errors\n",
    "                    print(f\"Error with {model_name} and features {features}: {e}\")\n",
    "                    continue\n",
    "    \n",
    "    # Convert to DataFrame and sort by test balanced accuracy\n",
    "    results_df = pd.DataFrame(results)\n",
    "    results_df = results_df.sort_values('test_balanced_acc', ascending=False)\n",
    "    \n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "33129307",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting comprehensive feature search...\n",
      "\n",
      "Total features available: 10\n",
      "Features: ['MMSE', 'CDRSUM', 'CDRGLOB', 'HVLT_DR', 'LASSI_A_CR2', 'LASSI_B_CR1', 'LASSI_B_CR2', 'APOE', 'AMYLPET', 'PTAU_217_CONCNTRTN']\n",
      "\n",
      "Testing 10 combinations with 9 features...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b0fc0a68f1846b4a05429dff0712722",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "9 features:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing 1 combinations with 10 features...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c431e6cc912045c691d98815d6e34861",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "10 features:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Run the comprehensive search\n",
    "print(\"Starting comprehensive feature search...\\n\")\n",
    "results_df = comprehensive_feature_search(\n",
    "    train_df=train_df,\n",
    "    test_df=test_df,\n",
    "    target_col='FL_UDSD_cat',\n",
    "    min_features=9,\n",
    "    max_features=10  # Use all features or set a limit like 5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c556597d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# group = results_df.groupby(by=['model', 'n_features']).get_group(('RandomForest', 2))\n",
    "# results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53190a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_results = results_df.loc[results_df.groupby(['model', 'n_features'])['test_f1_macro_score'].idxmax()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c9ec13",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e74ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_results(\n",
    "    results_df: pd.DataFrame,\n",
    "    output_dir: str,\n",
    "    sort_by: str = 'test_f1_macro',\n",
    "    ascending: bool = False\n",
    "):\n",
    "    \"\"\"\n",
    "    Save feature search results to separate Excel files for each model.\n",
    "    Each Excel file contains sheets organized by feature count.\n",
    "    \n",
    "    Args:\n",
    "        results_df: DataFrame containing feature search results from comprehensive_feature_search\n",
    "        output_dir: Directory where the Excel files will be saved\n",
    "        sort_by: Column name to sort results by (default: 'test_balanced_acc')\n",
    "        ascending: Sort order (default: False for descending)\n",
    "    \n",
    "    Returns:\n",
    "        None. Saves Excel files to disk.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create output directory if it doesn't exist\n",
    "    Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Get unique models\n",
    "    models = results_df['model'].unique()\n",
    "    \n",
    "    # Create a separate Excel file for each model\n",
    "    for model in models:\n",
    "        # Filter data for this model\n",
    "        model_df = results_df[results_df['model'] == model].copy()\n",
    "        \n",
    "        # Create file path\n",
    "        file_path = Path(output_dir) / f\"{model}.xlsx\"\n",
    "        \n",
    "        # Create Excel writer\n",
    "        with pd.ExcelWriter(file_path, engine='openpyxl') as writer:\n",
    "            # Get unique feature counts for this model\n",
    "            feature_counts = sorted(model_df['n_features'].unique())\n",
    "            \n",
    "            # Create a sheet for each feature count\n",
    "            for n_features in feature_counts:\n",
    "                # Filter data for this feature count\n",
    "                filtered_df = model_df[model_df['n_features'] == n_features].copy()\n",
    "                \n",
    "                if not filtered_df.empty:\n",
    "                    # Sort the filtered data\n",
    "                    filtered_df = filtered_df.sort_values(sort_by, ascending=ascending)\n",
    "                    \n",
    "                    # Create sheet name\n",
    "                    sheet_name = f\"{n_features}_features\"\n",
    "                    \n",
    "                    # Write to Excel\n",
    "                    filtered_df.to_excel(writer, sheet_name=sheet_name, index=False)\n",
    "            \n",
    "            # Also create a summary sheet with all results for this model\n",
    "            model_df_sorted = model_df.sort_values(sort_by, ascending=ascending)\n",
    "            model_df_sorted.to_excel(writer, sheet_name='All_Results', index=False)\n",
    "        \n",
    "        print(f\"Created {file_path} with {len(feature_counts)} feature count sheets\")\n",
    "    \n",
    "    print(f\"\\nSaved {len(models)} Excel files to {output_dir}\")\n",
    "    print(f\"Models: {', '.join(models)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81d55676",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_results(results_df, 'results/',sort_by='test_f1_macro_score', ascending=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "beam",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
